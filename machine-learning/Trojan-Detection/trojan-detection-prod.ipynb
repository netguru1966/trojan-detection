{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072c4ad6",
   "metadata": {},
   "source": [
    "<center><img src=\"images/trojan-horse.png\" alt=\"drawing\" width=\"7500\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"><span style=\"color:blue\"><h1>Trojan Detection through ML</h1></span></div>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<span style=\"color:blue\"><h2>Using Ensemble Learning</h2></span></div>\n",
    "\n",
    "Ensemble methods create a strong model by combining the predictions of multiple weak models (also known as weak learners or base estimators) that are built with a given dataset and a given learning algorithm.\n",
    "\n",
    "Three major kinds of meta-algorithms that aims at combining weak learners:\n",
    "\n",
    "- <span style=\"color:red\"><b>Bagging,</b></span> that often considers homogeneous weak learners, learns them independently from each other in parallel and combines them following some kind of deterministic averaging process\n",
    "- <span style=\"color:red\"><b>Boosting,</b></span> that often considers homogeneous weak learners, learns them sequentially in a very adaptative way (a base model depends on the previous ones) and combines them following a deterministic strategy\n",
    "- <span style=\"color:red\"><b>Stacking,</b></span> that often considers heterogeneous weak learners, learns them in parallel and combines them by training a meta-model to output a prediction based on the different weak models predictions\n",
    "\n",
    "----\n",
    "\n",
    "__Trojan Detection Dataset__\n",
    "\n",
    "In this project, the CulinaryML team will work with historical trojan detection data in the [Trojan Detection Dataset](https://www.kaggle.com/datasets/subhajournal/trojan-detection/code). The target field of the dataset (**Class**) is the outcome of detection: <span style=\"color:red\"><b>1 for Trojan and 0 for Benign.</b></span> Multiple features are used in the dataset.\n",
    "\n",
    "__Dataset schema:__\n",
    "<span style=\"color:blue\">\n",
    "- __ID:__ Unique ID of the Packet\n",
    "- __Flow ID:__ Unique ID of the Packet Flow\n",
    "- __Source IP:__ Source IP address\n",
    "- __Source Port:__ Source TCP/User Datagram Protocol (UDP) ports\n",
    "- __Destination IP:__ Destination IP address\n",
    "- __Destination Port:__ Destination TCP/User Datagram Protocol (UDP) ports\n",
    "- __Protocol:__ TCP flags and encapsulated protocol (TCP/UDP)\n",
    "- __Flow Duration:__ Duration of Packet Flow\n",
    "- __Total Fwd Packets:__ Number of Forward Packets\n",
    "- __Total Backward Packets:__  Number of Backward Packet\n",
    "- __Total Length of Fwd Packets:__ Length of Forward Packet\n",
    "- __Total Length of Bwd Packets:__ Length of Backward Packet\n",
    "- __Fwd Packet Length Max:__ Length of Forward Packet (Max)\n",
    "- __Fwd Packet Length Min:__ Length of Forward Packet (Min)\n",
    "- __Fwd Packet Length Mean:__ Length of Forward Packet (Mean)\n",
    "- __Fwd Packet Length Std:__ Length of Forward Packet (STD)\n",
    "- __Bwd Packet Length Max:__ Length of Backward Packet (Max)\n",
    "- __Bwd Packet Length Min:__ Length of Backward Packet (Min)\n",
    "- __Bwd Packet Length Mean:__ Length of Backward Packet (Mean)\n",
    "- __Bwd Packet Length Std:__ Length of Backward Packet (STD)\n",
    "- __Fwd IAT Total:__ IAT Total\n",
    "- __Fwd Header Length:__ Length of Forward Header\n",
    "- __Bwd Header Length:__ Length of Backward Header\n",
    "- __Min Packet Length:__ Packet Length (Min)\n",
    "- __Max Packet Length:__ Packet Length (Max)\n",
    "- __Packet Length Mean:__ Packet Length (Mean)\n",
    "- __Packet Length Std:__ Packet Length (STD)\n",
    "- __Packet Length Variance:__ Packet Length (Variance)\n",
    "- __Average Packet Size:__ Packet Size\n",
    "- __Avg Fwd Segment Size:__ Forward Segment Size\n",
    "- __Avg Bwd Segment Size:__ Backward Segment Size\n",
    "- __Fwd Header Length.1:__ Forward Header Leader\n",
    "- __Class:__ Trojan or Benign\n",
    "</span>\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d633e36",
   "metadata": {},
   "source": [
    "<center><img src=\"images/culinaryML.png\" alt=\"drawing\" width=\"500\" style=\"background-color:white; padding:1em;\" /></center>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h1><span style=\"color:blue\">CulinaryML Process</span></h1></div>\n",
    "\n",
    "- [Data Collection](#Data-Collection)\n",
    "- [Feature Engineering](#Feature-Engineering)\n",
    "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "- [Data Preparation](#Data-Preparation)\n",
    "- [Model Building](#Model-Building)\n",
    "- [Bagging](#Bagging)\n",
    "- [Random Forest](#Random-Forest)\n",
    "- [Boosting](#Boosting)\n",
    "- [Model Evaluation](#Model-Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e7ae45",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h1><span style=\"color:blue\"> Data Collection</span></h1></div>\n",
    "\n",
    "Before CulinaryML builds a model, we need to collect the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb0e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install libraries\n",
    "!pip install -U -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d4bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0d6af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Trojan_Detection_modified.csv\",sep=\",\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "print(\"The shape of the dataset is:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6151fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ecf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"Benign\": 0, \"Trojan\": 1}\n",
    "df['Malware Type'] = df['Class'].map(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b7488",
   "metadata": {},
   "source": [
    "Convert Propose Target Feature **(Class)** to Binary and Rename to **\"Malware Type\"** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c176b9",
   "metadata": {},
   "source": [
    "d = {\"Benign\": 0, \"Trojan\": 1}\n",
    "df[\"Malware Type\"] = df[\"Class\"].map(d)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91acaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Source IP'] = df['Source IP'].str.replace('.','')\n",
    "df['Destination IP'] = df['Destination IP'].str.replace('.','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915dc75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87340528",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b203f4",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h1><span style=\"color:blue\">Feature Engineering</span></h1></div>\n",
    "\n",
    "Remove unnecessary features (columns) to minimize under-fitting and over-fitting\n",
    "\n",
    "We used some commands including the number of rows, number of columns, and some simple statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef87337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "df.drop([\n",
    "    \"Flow ID\",\n",
    "    \"Total Backward Packets\",\n",
    "    \"Total Fwd Packets\",\n",
    "    \"Total Backward Packets\",\n",
    "    \"Total Length of Fwd Packets\",\n",
    "    \"Total Length of Bwd Packets\",\n",
    "    \"Bwd Packet Length Max\",\n",
    "    \"Bwd Packet Length Min\",\n",
    "    \"Bwd Packet Length Mean\",\n",
    "    \"Bwd Packet Length Std\",\n",
    "    'Fwd Packet Length Min',\n",
    "    'Fwd Packet Length Max',\n",
    "    'Fwd Packet Length Mean',\n",
    "    'Fwd Packet Length Std',\n",
    "    'Fwd Packet Length Min',\n",
    "    'Fwd Packet Length Max',\n",
    "    'Fwd Packet Length Mean',\n",
    "    'Fwd Packet Length Std',\n",
    "    'Fwd IAT Total',\n",
    "    'Fwd Header Length',\n",
    "    'Bwd Header Length',\n",
    "    'Min Packet Length',\n",
    "    'Max Packet Length',\n",
    "    'Packet Length Mean',\n",
    "    'Packet Length Std',\n",
    "    'Packet Length Variance',\n",
    "    'Average Packet Size',\n",
    "    'Avg Fwd Segment Size',\n",
    "    'Avg Bwd Segment Size',\n",
    "    'Fwd Header Length.1',\n",
    "'Class'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63b3ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b54561b",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h1><span style=\"color:blue\">Exploratory Data Analysis (EDA)</span></h1></div>\n",
    "\n",
    "CulinaryML takes an analysis approach that identifies general patterns in the data. These patterns include outliers and features of the data that might be unexpected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de3954",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df['Malware Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Malware distribution from the Trojan Detection set:\")\n",
    "print(df['Malware Type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39918cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def le(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "                label_encoder = LabelEncoder()\n",
    "                df[col] = label_encoder.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce7e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd441b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create lists of the features and name the target\n",
    "\n",
    "# Numerical features \n",
    "numerical_features = [\n",
    "    'Flow Duration',\n",
    "    'Source Port',\n",
    "    'Destination Port',\n",
    "    'Protocol' \n",
    "]\n",
    "\n",
    "# Based on exploratory data analysis (EDA), select the categorical features\n",
    "categorical_features = [\n",
    "                 'Source IP',\n",
    "                 'Destination IP'\n",
    "                              \n",
    "]\n",
    "\n",
    "model_features = numerical_features + categorical_features\n",
    "model_target = ['Malware Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280dc1b",
   "metadata": {},
   "source": [
    "To review the numerical features, use the `value_counts()` function to get a view of the feature values in respective bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d7315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print and plot statistics for the numerical features\n",
    "for c in numerical_features:\n",
    "    # Print the name of the feature\n",
    "    print(c)\n",
    "    # Print the value counts in 10 bins for each feature\n",
    "    print(df[c].value_counts(bins=10, sort=False))\n",
    "\n",
    "    # Plot bar charts based on value_counts (alternative plot method)\n",
    "    df[c].value_counts(bins=10, sort=False).plot(kind=\"bar\", alpha=0.75, rot=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b6ddbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[model_features].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e143a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[model_target].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d917f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd7f1ef",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h1><span style=\"color:blue\">Data Processing</span></h1></div>\n",
    "\n",
    "Next need to import and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[model_features]\n",
    "y = df[model_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b525ccf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e078ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca618798",
   "metadata": {},
   "source": [
    "The data is now prepared, and you are ready to create a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ca684",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1><span style=\"color:blue\">Model Selection</span></h1></div>\n",
    "\n",
    "To train a logistic regression model, you will use sklearn's [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f14ebfd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h3><span style=\"color:blue\">Adoption Classification</span></h3></div>\n",
    "\n",
    "   (**Malware Type**) is the outcome of adoption:<span style=\"color:red\"><b> 1 for Trojan and 0 for not Benign.</b></span>\n",
    "\n",
    "<center><img src=\"images/logistic_function.png\" alt=\"drawing\" width=\"800\" style=\"background-color:white; padding:1em;\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Data into One Dimension Array for easy processing.\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4112868d",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h3><span style=\"color:blue\">Performance</span></h3></div>\n",
    "\n",
    "   (**Malware Type**) is the outcome of adoption:<span style=\"color:red\"><b> 1 for Trojan and 0 for not Benign.</b></span>\n",
    "\n",
    "<center><img src=\"images/performance.png\" alt=\"drawing\" width=\"800\" style=\"background-color:white; padding:1em;\" /></center>\n",
    "\n",
    "\n",
    "---\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h3><span style=\"color:blue\">Process the data using Cross Validation with GridSearchCV, and RandomSearchCV</span></h3></div>\n",
    "\n",
    "In <span style=\"color:red\"><b><span>GridSearchCV,</b></span> along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model. As we know that before training the model with data, we divide the data into two parts – train data and test data. In cross-validation, the process divides the train data further into two parts – the train data and the validation data.\n",
    "\n",
    "The most popular type of Cross-validation is K-fold Cross-Validation. It is an iterative process that divides the train data into k partitions. Each iteration keeps one partition for testing and the remaining k-1 partitions for training the model. The next iteration will set the next partition as test data and the remaining k-1 as train data and so on. In each iteration, it will record the performance of the model and at the end give the average of all the performance. \n",
    "\n",
    "Primarily, it takes 4 arguments i.e. estimator, param_grid, cv, and scoring. The description of the arguments is as follows:\n",
    "\n",
    "**1. estimator** – A scikit-learn model\n",
    "\n",
    "**2. param_grid** – A dictionary with parameter names as keys and lists of parameter values.\n",
    "\n",
    "**3. scoring** – The performance measure. For example, ‘r2’ for regression models, ‘precision’ for classification models.\n",
    "\n",
    "**4. cv** – An integer that is the number of folds for K-fold cross-validation.\n",
    "\n",
    "<span style=\"color:red\"><b><span>GridSearchCV</b></span> can be used on several hyperparameters to get the best values for the specified hyperparameters.\n",
    "    \n",
    "Random search cross-validation <span style=\"color:red\"><b><span>(RandomizedSearchCV)</b></span> is another powerful technique for optimizing the hyperparameters of a machine learning model. It works in a similar way to grid search cross-validation, but instead of searching over a predefined grid of hyperparameters, it samples them randomly from a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dependencies\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0f540",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h3><span style=\"color:blue\">GridSearchCV</span></h3></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c529175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the SVC model\n",
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161098d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "parameters = {\n",
    "              'kernel':['linear','poly','rbf','sigmoid'],\n",
    "              'C':[1, 5, 10, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n",
    "clf = GridSearchCV(model, parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec15418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the data to our model\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758dd017",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5544e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters\n",
    "\n",
    "best_parameters = clf.best_params_\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# higest accuracy\n",
    "\n",
    "highest_accuracy = clf.best_score_\n",
    "print(highest_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e444625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the results to pandas dataframe\n",
    "result = pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5346f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e47419",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_result = result[['param_C','param_kernel','mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbb254",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8542241f",
   "metadata": {},
   "source": [
    "Highest Accuracy = 84%\n",
    "\n",
    "Best Parameters = {'C':1, 'kernel':'sigmoid'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092df662",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h3><span style=\"color:blue\">RandomSearchCV</span></h3></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the SVC model\n",
    "model = SVC()\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "parameters = {\n",
    "              'kernel':['linear','poly','rbf','sigmoid'],\n",
    "              'C':[1, 5, 10, 20]\n",
    "}\n",
    "\n",
    "# Baseline grid search\n",
    "clf = RandomizedSearchCV(model, parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the data to our model\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b527e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomizedSearchCV(cv=5, estimator=SVC(),\n",
    "                   param_distributions={'C': [1, 5, 10, 20],\n",
    "                                        'kernel': ['linear', 'poly', 'rbf',\n",
    "                                                   'sigmoid']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b577df",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba015a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters\n",
    "\n",
    "best_parameters = clf.best_params_\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# higest accuracy\n",
    "\n",
    "highest_accuracy = clf.best_score_\n",
    "print(highest_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531be8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the results to pandas dataframe\n",
    "result = pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f27317",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_search_result = result[['param_C','param_kernel','mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d6b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef8754",
   "metadata": {},
   "source": [
    "Highest Accuracy = 84%\n",
    "\n",
    "Best Parameters = {'C':1, 'kernel':'sigmoid'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096c0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of baseline models\n",
    "models = [LogisticRegression(max_iter=1000), SVC(kernel='linear'), KNeighborsClassifier(), RandomForestClassifier(random_state=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd32f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_cross_validation():\n",
    "\n",
    "  for model in models:\n",
    "\n",
    "    cv_score = cross_val_score(model, X, y, cv=5)\n",
    "    mean_accuracy = sum(cv_score)/len(cv_score)\n",
    "    mean_accuracy = mean_accuracy*100\n",
    "    mean_accuracy = round(mean_accuracy, 2)\n",
    "\n",
    "    print('Cross Validation accuracies for the',model,'=', cv_score)\n",
    "    print('Acccuracy score of the ',model,'=',mean_accuracy,'%')\n",
    "    print('---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95330332",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models_cross_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046c6bb",
   "metadata": {},
   "source": [
    "Inference: For the Heart Disease dataset, Random Forest Classifier has the Highest accuracy value with default hyperparameter values\n",
    "\n",
    "    Comparing the models with different Hyperparameter values using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c981ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of models\n",
    "models_list = [LogisticRegression(max_iter=10000), SVC(), KNeighborsClassifier(), RandomForestClassifier(random_state=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45de51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary that contains hyperparameter values for the above mentioned models\n",
    "\n",
    "\n",
    "model_hyperparameters = {\n",
    "    \n",
    "\n",
    "    'log_reg_hyperparameters': {\n",
    "        \n",
    "        'C' : [1,5,10,20]\n",
    "    },\n",
    "\n",
    "    'svc_hyperparameters': {\n",
    "        \n",
    "        'kernel' : ['linear','poly','rbf','sigmoid'],\n",
    "        'C' : [1,5,10,20]\n",
    "    },\n",
    "\n",
    "\n",
    "    'KNN_hyperparameters' : {\n",
    "        \n",
    "        'n_neighbors' : [3,5,10]\n",
    "    },\n",
    "\n",
    "\n",
    "    'random_forest_hyperparameters' : {\n",
    "        \n",
    "        'n_estimators' : [10, 20, 50, 100]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aaa827",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_hyperparameters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f7141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hyperparameters['log_reg_hyperparameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keys = list(model_hyperparameters.keys())\n",
    "print(model_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f19b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfbc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hyperparameters[model_keys[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee8c647",
   "metadata": {},
   "source": [
    "Applying GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1179944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelSelection(list_of_models, hyperparameters_dictionary):\n",
    "\n",
    "  result = []\n",
    "\n",
    "  i = 0\n",
    "\n",
    "  for model in list_of_models:\n",
    "\n",
    "    key = model_keys[i]\n",
    "\n",
    "    params = hyperparameters_dictionary[key]\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    print(model)\n",
    "    print(params)\n",
    "    print('---------------------------------')\n",
    "\n",
    "\n",
    "    clf = GridSearchCV(model, params, cv=5)\n",
    "\n",
    "    # fitting the data to classifier\n",
    "    clf.fit(X,y)\n",
    "\n",
    "    result.append({\n",
    "        'model used' : model,\n",
    "        'highest score' : clf.best_score_,\n",
    "        'best hyperparameters' : clf.best_params_\n",
    "    })\n",
    "\n",
    "  result_dataframe = pd.DataFrame(result, columns = ['model used','highest score','best hyperparameters'])\n",
    "\n",
    "  return result_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelSelection(models_list, model_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b22a35",
   "metadata": {},
   "source": [
    "Random Forest Classifier with n_estimators = 100 has the highest accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e705a8b",
   "metadata": {},
   "source": [
    "Finally, train the classifier with <span style=\"color:red\"><b>.fit()</b></span> on the training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c7650",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1><span style=\"color:blue\">Model Training</span></h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e890ac",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h3><span style=\"color:blue\">Create training and test datasets</span></h3></div>\n",
    "\n",
    "\n",
    "As part of data preparation, the dataset is split into training and test subsets by using sklearn's [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function.\n",
    "\n",
    "For this notebook, you will use 80 percent of the data for the training set and 20 percent for the test set. Determine the best split based on the size of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the results to pandas dataframe\n",
    "X = df[model_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644953d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[model_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951679b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `replace()` function to remove commas\n",
    "df['Destination IP'] = df['Destination IP'].replace('/d.', '', regex=True)\n",
    "\n",
    "# Convert the column to floats\n",
    "#df['Source IP','Destination IP'] = df['Source IP','Destination IP'].astype(float)\n",
    "\n",
    "# Print the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646c762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6776233",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9074b47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc393270",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cf6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b36579",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03059d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db1464",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c135f90d",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h3><span style=\"color:blue\">Process the data with a pipeline and ColumnTransformer</span></h3></div>\n",
    "\n",
    "In a typical ML workflow, you need to apply data transformations, such as imputation and scaling, at least twice: first on the training dataset by using <span style=\"color:red\"><b>.fit()</b></span> and <span style=\"color:red\"><b><span>.transform()</b></span> when preparing the data to train the model, and then by using <span style=\"color:red\"><b><span>.transform()</b></span> on any new data that you want to predict on (validation or test). Sklearn's [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) is a tool that simplifies this process by enforcing the implementation and order of data processing steps, being important for reproducibility. In other words, all the data is transformed the same way each time that you process any part of it.\n",
    "\n",
    "In this section, you will build separate pipelines to handle the numerical, and categorical features. Then, you will combine them into a composite pipeline along with an estimator. To do this, you will use a [LogisticRegression classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "You will need multiple pipelines to ensure that all the data is handled correctly:\n",
    "\n",
    "* __Numerical features pipeline:__ Impute missing values with the mean by using sklearn's SimpleImputer, followed by a MinMaxScaler. If different processing is desired for different numerical features, different pipelines should be built as described for the text features pipeline. See the <span style=\"color:red\"><b><span>numerical_processor</b></span> in the following code cell.\n",
    "\n",
    "* __Categoricals pipeline:__ Impute with a placeholder value (this won't have an effect because you already encoded the 'nan' values), and encode with sklearn's OneHotEncoder. If computing memory is an issue, it is a good idea to check the number of unique values for the categoricals to get an estimate of how many dummy features one-hot encoding will create. Note the <span style=\"color:red\"><b><span>handle_unknown</b></span> parameter, which tells the encoder to ignore (rather than throw an error for) any unique value that might show in the validation or test set that was not present in the initial training set. See the <span style=\"color:red\"><b><span>categorical_processor</b></span> in the following code cell.\n",
    "\n",
    "Finally, the selective preparations of the dataset features are then put together into a collective ColumnTransformer, which is used in a pipeline along with an estimator. This ensures that the transforms are performed automatically in all situations. This includes on the raw data when fitting the model, when making predictions, when evaluating the model on a validation dataset through cross-validation, or when making predictions on a test dataset in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49545d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
    "\n",
    "# The pipeline can be used as any other estimator\n",
    "# and avoids leaking the test set into the train set\n",
    "pipeline.fit(X_train, y_train).score(X_test, y_test)\n",
    "# An estimator's parameter can be set using '__' syntax\n",
    "\n",
    "pipeline.set_params(svc__C=10).fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd83467",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h1><span style=\"color:blue\">Data Modeling</span></h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cfdf01",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2><span style=\"color:blue\">Bagging</span></h2></div>\n",
    "\n",
    "\n",
    "   (**Malware Type**) is the outcome of adoption:<span style=\"color:red\"><b> 1 for trojan and 0 for benign.</b></span>\n",
    "\n",
    "<center><img src=\"Hyperparameter_Optimization_using_Grid_Search.svg.png\" alt=\"drawing\" width=\"800\" style=\"background-color:white; padding:1em;\" /></center>\n",
    "\n",
    "In this section, you will build your first ensemble model by using the bootstrap aggregating, or bagging, approach. With this approach, you randomly draw multiple data subsets from the training set (with replacement) and train one model for each subset.\n",
    "\n",
    "The first approach will use multiple trees in the bagging model.\n",
    "\n",
    "---\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h3><span style=\"color:blue\">Data processing with a pipeline and a bagging ColumnTransformer</span></h3></div>\n",
    "\n",
    "\n",
    "You need to use different pipelines to handle the numerical, categorical, and text features. Then, you will combine them into a composite pipeline along with an estimator. To do this, you will use a [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4efaee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "\n",
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"num_scaler\",\n",
    "            MinMaxScaler(),\n",
    "        )  # Shown in case it is needed. Not a must with decision trees.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the categorical features\n",
    "# handle_unknown tells it to ignore (rather than throw an error for) any value\n",
    "# that was not present in the initial training set.\n",
    "\n",
    "#categorical_processor = Pipeline(\n",
    "#    [(\"cat_encoder\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
    "#)\n",
    "\n",
    "\n",
    "# Combine all data preprocessors from above (add more if you choose to define more)\n",
    "# For each processor/step, specify a name, the actual process, and the features to be processed\n",
    "data_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical_pre\", numerical_processor, numerical_features),\n",
    "        (\"categorical_pre\", categorical_processor, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "### PIPELINE ###\n",
    "################\n",
    "\n",
    "# Pipeline with all desired data transformers, along with an estimator\n",
    "# Later, you can set/reach the parameters by using the names issued - for hyperparameter tuning, for example\n",
    "\n",
    "#####################################################\n",
    "### Notice the pipeline using a BaggingClassifier ###\n",
    "#####################################################\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"data_preprocessing\", data_preprocessor),\n",
    "        (\n",
    "            \"bg\",\n",
    "            BaggingClassifier(\n",
    "                DecisionTreeClassifier(max_depth=25),  # Each tree has max_depth=25\n",
    "                n_estimators=10,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")  # Use 10 trees\n",
    "\n",
    "# Visualize the pipeline\n",
    "# This will be helpful especially when building more complex pipelines,\n",
    "# stringing together multiple preprocessing steps\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e31ccb",
   "metadata": {},
   "source": [
    "Now you can fit the bagging model, and see the training and test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2530919a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get training data to train the pipeline\n",
    "# Get testing data to test the pipeline\n",
    "X_train = df[model_features]\n",
    "y_test = df[model_target]\n",
    "\n",
    "# The pipeline can be used as any other estimator\n",
    "# and avoids leaking the test set into the train set\n",
    "pipeline.fit(X_train, y_train).score(X_train, y_train)\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the training dataset\n",
    "train_predictions = pipeline.predict(X_train)\n",
    "print(confusion_matrix(y_train, train_predictions))\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"Accuracy (training):\", accuracy_score(y_train, train_predictions))\n",
    "\n",
    "# Get testing data to test the pipeline\n",
    "#X_test = test_data[model_features]\n",
    "#y_test = test_data[model_target]\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the testing dataset\n",
    "#test_predictions = pipeline.predict(X_test)\n",
    "#print(confusion_matrix(y_test, test_predictions))\n",
    "#print(classification_report(y_test, test_predictions))\n",
    "#print(\"Accuracy (test):\", accuracy_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d055f82c",
   "metadata": {},
   "source": [
    "<center><img src=\"Confusion Matrix.png\" alt=\"drawing\" width=\"500\" style=\"background-color:white; padding:1em;\" /></center>\n",
    "\n",
    "\n",
    "Using a bagging classifier isn't difficult because it only requires updating one line of code.\n",
    "\n",
    "Next, you will create a random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e74123",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h1><span style=\"color:blue\">Random forest</span></h1></div>\n",
    "\n",
    "\n",
    "Now, you will try the second ensemble model: random forest. Random forest involves a similar ensemble process:\n",
    "- Draw random subsets (with replacement) from the original dataset.\n",
    "- Train individual trees with each subset.\n",
    "\n",
    "However, a difference is that random forest uses a randomly selected feature subset for each tree. As a rule of thumb, pick the `sqrt(# features)` as the number of random features for each tree and don't use any other features.\n",
    "\n",
    "\n",
    "The model is called in a similar way to the bagging method. You will replace the BaggingClassifier with a [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf780cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"num_scaler\",\n",
    "            MinMaxScaler(),\n",
    "        )  # Shown in case it is needed. Not a must with decision trees.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the categorical features\n",
    "# handle_unknown tells it to ignore (rather than throw an error for) any value\n",
    "# that was not present in the initial training set.\n",
    "\n",
    "#categorical_processor = Pipeline(\n",
    "#    [(\"cat_encoder\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
    "#)\n",
    "\n",
    "# Preprocess the text feature\n",
    "text_processor_0 = Pipeline(\n",
    "    [(\"text_vect_0\", CountVectorizer(binary=True, max_features=150))]\n",
    ")\n",
    "\n",
    "# Combine all data preprocessors (add more if you choose to define more)\n",
    "# For each processor/step, specify a name, the actual process, and the features to be processed\n",
    "data_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical_pre\", numerical_processor, numerical_features),\n",
    "#        (\"categorical_pre\", categorical_processor, categorical_features),\n",
    "        (\"text_pre_0\", text_processor_0, text_features[0]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "### PIPELINE ###\n",
    "################\n",
    "\n",
    "# Pipeline with all desired data transformers, along with an estimator\n",
    "# Later, you can set/reach the parameters by using the names issued - for hyperparameter tuning, for example\n",
    "\n",
    "##########################################################\n",
    "### Notice the pipeline using a RandomForestClassifier ###\n",
    "##########################################################\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"data_preprocessing\", data_preprocessor),\n",
    "        (\n",
    "            \"rf\",\n",
    "            RandomForestClassifier(\n",
    "                max_depth=25, n_estimators=100  # Each tree has max_depth=25\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")  # Use 100 trees\n",
    "\n",
    "# Visualize the pipeline\n",
    "# This will be helpful especially when building more complex pipelines,\n",
    "# stringing together multiple preprocessing steps\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9decbf5",
   "metadata": {},
   "source": [
    "Now you can fit the random forest model, and see the training and test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec7be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get training data to train the pipeline\n",
    "X_train = train_data[model_features]\n",
    "y_train = train_data[model_target]\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the training dataset\n",
    "train_predictions = pipeline.predict(X_train)\n",
    "print(confusion_matrix(y_train, train_predictions))\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"Accuracy (training):\", accuracy_score(y_train, train_predictions))\n",
    "\n",
    "# Get testing data to test the pipeline\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the testing dataset\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "print(confusion_matrix(y_test, test_predictions))\n",
    "print(classification_report(y_test, test_predictions))\n",
    "print(\"Accuracy (test):\", accuracy_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd91aa1d",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h3><i>Try it yourself!</i></h3>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">You can perform hyperparameter tuning on a random forest model.</p><br>\n",
    "    <p style=\" text-align: center; margin: auto;\">In the following code cell, run a grid search with the random forest classifier using <code>param_grid={'rf__max_depth': [25, 30, 45]}</code>.</p><br>\n",
    "    <p style=\" text-align: center; margin: auto;\">What is the best hyperparameter value after this run?</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e489f9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your code for grid search with param_grid={'rf__max_depth': [25, 30, 45]}\n",
    "\n",
    "# Parameter grid for GridSearch\n",
    "\n",
    "############### CODE HERE ###############\n",
    "\n",
    "from scipy.stats import randint\n",
    "# Parameter grid for GridSearch\n",
    "\n",
    "param_grid = {\n",
    "    'rf__max_depth': [25, 30, 45]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,  # Base model\n",
    "    param_grid,  # Parameters to try\n",
    "    cv=5,  # Apply 5-fold cross validation\n",
    "    verbose=1,  # Print summaryGridSearchCV\n",
    "    n_jobs=-1,  # Use all available processors\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearch to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "############## END OF CODE ##############\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Get the best model out of GridSearchCV\n",
    "classifier = grid_search.best_estimator_\n",
    "\n",
    "# Fit the best model to the training data\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdceaa95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get testing data to test the classifier\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "# Use the fitted model to make predictions on the test dataset\n",
    "# Testing data going through the pipeline is first imputed\n",
    "# (with means from the training set), scaled (with the min/max from the training data),\n",
    "# and finally used to make predictions.\n",
    "test_predictions = classifier.predict(X_test)\n",
    "\n",
    "print(\"Model performance on the test set:\")\n",
    "print(confusion_matrix(y_test, test_predictions))\n",
    "print(classification_report(y_test, test_predictions))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba308da",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h1><span style=\"color:blue\">Boosting</span></h1></div>\n",
    "\n",
    "The last ensemble model that you will try is boosting. This method builds multiple weak models sequentially. Each subsequent model attempts to boost performance overall by overcoming or reducing the errors of the previous model.\n",
    "\n",
    "You will use sklearn's [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871cd199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"num_scaler\",\n",
    "            MinMaxScaler(),\n",
    "        )  # Shown in case it is needed. Not a must with decision trees.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the categorical features\n",
    "# handle_unknown tells it to ignore (rather than throw an error for) any value\n",
    "# that was not present in the initial training set.\n",
    "\n",
    "#categorical_processor = Pipeline(\n",
    "#    [(\"cat_encoder\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
    "#)\n",
    "\n",
    "# Preprocess the text feature\n",
    "text_processor_0 = Pipeline(\n",
    "    [(\"text_vect_0\", CountVectorizer(binary=True, max_features=150))]\n",
    ")\n",
    "\n",
    "# Combine all data preprocessors (add more if you choose to define more)\n",
    "# For each processor/step, specify a name, the actual process, and the features to be processed\n",
    "data_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical_pre\", numerical_processor, numerical_features),\n",
    "#        (\"categorical_pre\", categorical_processor, categorical_features),\n",
    "        (\"text_pre_0\", text_processor_0, text_features[0]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "### PIPELINE ###\n",
    "################\n",
    "\n",
    "# Pipeline with all desired data transformers, along with an estimator\n",
    "# Later, you can set/reach the parameters by using the names issued - for hyperparameter tuning, for example\n",
    "\n",
    "##############################################################\n",
    "### Notice the pipeline using a GradientBoostingClassifier ###\n",
    "##############################################################\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"data_preprocessing\", data_preprocessor),\n",
    "        (\n",
    "            \"gbc\",\n",
    "            GradientBoostingClassifier(\n",
    "                max_depth=10, n_estimators=100  # Each tree has max_depth=10\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")  # Use 100 trees\n",
    "\n",
    "# Visualize the pipeline\n",
    "# This will be helpful especially when building more complex pipelines,\n",
    "# stringing together multiple preprocessing steps\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0107a449",
   "metadata": {},
   "source": [
    "Now fit the model, and see the training and testing scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa4049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get training data to train the pipeline\n",
    "X_train = train_data[model_features]\n",
    "y_train = train_data[model_target]\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the training dataset\n",
    "train_predictions = pipeline.predict(X_train)\n",
    "print(confusion_matrix(y_train, train_predictions))\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"Accuracy (training):\", accuracy_score(y_train, train_predictions))\n",
    "\n",
    "# Get testing data to test the pipeline\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the testing dataset\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "print(confusion_matrix(y_test, test_predictions))\n",
    "print(classification_report(y_test, test_predictions))\n",
    "print(\"Accuracy (test):\", accuracy_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc5ded",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h1><span style=\"color:blue\">Conclusion</span></h1></div>\n",
    "\n",
    "This notebook provided an introduction to using Bagging, RandomForest, and GradientBoosting classifiers on the same dataset.\n",
    "\n",
    "---\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h1><span style=\"color:blue\">Next Lab</span></h1></div>\n",
    "\n",
    "In the next lab, you will be introduced to fairness and bias mitigation in ML by exploring different types of bias that are present in data and practicing how to build various documentation sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3306a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
